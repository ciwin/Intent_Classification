{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Dark-Sied/Intent_Classification/blob/master/Intent_classification_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intent Recognition Dolores V01\n",
    "Using a new vocabulary and training set.  \n",
    "Intent Recognition is based on Intent_classification_final\n",
    "Created by Christoph Windheuser, April 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "a_WypuUXi92e",
    "outputId": "133d026e-4236-4ff6-f21d-739bfb9640db"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "import nltk\n",
    "import re\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, LSTM, Bidirectional, Embedding, Dropout\n",
    "from keras.callbacks import ModelCheckpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE GLOBAL VARIABLES:\n",
    "NUM_SENT = 0\n",
    "NUM_INTENTS = 0\n",
    "NUM_INTENTS_UNIQUE = 0\n",
    "VOCABULARY_SIZE = 0\n",
    "MAX_SENT_LENGTH = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUM_SENT:            346\n",
      "NUM_INTENTS:         346\n",
      "NUM_INTENTS_UNIQUE:  14\n",
      "VOCABULARY_SIZE:     284\n",
      "MAX_SENT_LENGTH:     12\n"
     ]
    }
   ],
   "source": [
    "# SHOW GLOBAL VARIABLES\n",
    "print (\"NUM_SENT:           \", NUM_SENT)\n",
    "print (\"NUM_INTENTS:        \", NUM_INTENTS)\n",
    "print (\"NUM_INTENTS_UNIQUE: \", NUM_INTENTS_UNIQUE)\n",
    "print (\"VOCABULARY_SIZE:    \", VOCABULARY_SIZE)\n",
    "print (\"MAX_SENT_LENGTH:    \", MAX_SENT_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LE6wywJrN2ih"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Sentence           Intent\n",
      "0         are you a machine?  are_you_a_robot\n",
      "1  how are the things going?      how_are_you\n",
      "2             nah not for me             deny\n",
      "3           What's going on?      how_are_you\n",
      "4             are you a bot?  are_you_a_robot\n"
     ]
    }
   ],
   "source": [
    "# df = pd.read_csv(\"Dolores_Dataset_v01.csv\", encoding = \"latin1\", names = [\"Sentence\", \"Intent\"])\n",
    "df = pd.read_csv(\"mowgli_train_new.csv\", encoding = \"latin1\", names = [\"Sentence\", \"Intent\"])\n",
    "print(df.head())\n",
    "intents            = df[\"Intent\"]\n",
    "NUM_INTENTS        = len(list(df[\"Intent\"]))\n",
    "intents_unique     = list(set(df[\"Intent\"]))\n",
    "NUM_INTENTS_UNIQUE = len(intents_unique)\n",
    "sentences          = list(df[\"Sentence\"])\n",
    "NUM_SENT           = len(sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "tF0FQA7gjOCX",
    "outputId": "c609b42a-05da-49f5-8d11-bd670210f635",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Intents: \n",
      "['leave_budget', 'deny', 'goodbye', 'thanks', 'confirm', 'sorry', 'what_is_your_name', 'are_you_a_robot', 'insult', 'how_are_you', 'greet', 'conversation_restart', 'skills', 'personal_question']\n",
      "Num of unique Intents:  14\n"
     ]
    }
   ],
   "source": [
    "print (\"Unique Intents: \")\n",
    "print (intents_unique)\n",
    "print (\"Num of unique Intents: \", len(intents_unique))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Intent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>are you a machine?</td>\n",
       "      <td>are_you_a_robot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>how are the things going?</td>\n",
       "      <td>how_are_you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nah not for me</td>\n",
       "      <td>deny</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What's going on?</td>\n",
       "      <td>how_are_you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>are you a bot?</td>\n",
       "      <td>are_you_a_robot</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Sentence           Intent\n",
       "0         are you a machine?  are_you_a_robot\n",
       "1  how are the things going?      how_are_you\n",
       "2             nah not for me             deny\n",
       "3           What's going on?      how_are_you\n",
       "4             are you a bot?  are_you_a_robot"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(346, 2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "O8LLUZlokg0S",
    "outputId": "c15c21dc-2ef2-43b7-b4af-e7ee9e014091"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['are you a machine?', 'how are the things going?', 'nah not for me', \"What's going on?\", 'are you a bot?']\n"
     ]
    }
   ],
   "source": [
    "print(sentences[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Cleaning\n",
    "re.sub is a routine from the \"Regular Expression\" Library.     \n",
    "r'string' means that this is a \"raw string\", where backslashes are treated as charachters.    \n",
    "re.sub(r'[^ a-z A-Z 0-9]', \" \", s) means that all characters exept a-z, A-Z and 0-9 will be replaced by space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j-7q3iG5PKYI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['are', 'you', 'a', 'machine'], ['how', 'are', 'the', 'things', 'going'], ['nah', 'not', 'for', 'me'], ['what', 's', 'going', 'on'], ['are', 'you', 'a', 'bot']]\n",
      "Len of clean_sent:  346\n"
     ]
    }
   ],
   "source": [
    "clean_sent = []\n",
    "for s in sentences:\n",
    "    clean = re.sub(r'[^ a-z A-Z 0-9]', \" \", s)\n",
    "    w = word_tokenize(clean)\n",
    "    clean_sent.append([i.lower() for i in w])\n",
    "\n",
    "print (clean_sent[:5])\n",
    "print(\"Len of clean_sent: \", len(clean_sent))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Documentation for Tokenizer:\n",
    "https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SJCQ_YhBJW7t"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sent length:  12\n"
     ]
    }
   ],
   "source": [
    "token = Tokenizer(filters = '!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~')\n",
    "token.fit_on_texts(clean_sent)\n",
    "word_index = token.word_index\n",
    "VOCABULARY_SIZE = len(word_index) + 1\n",
    "MAX_SENT_LENGTH = len(max(clean_sent, key = len))\n",
    "\n",
    "#print(\"Vocab Size = %d. Maximum sent length = %d\" % (vocab_size, max_sent_length))\n",
    "print (\"Max sent length: \", MAX_SENT_LENGTH)\n",
    "encoded_sent = token.texts_to_sequences(clean_sent)\n",
    "padded_sent = pad_sequences(encoded_sent, maxlen = MAX_SENT_LENGTH, padding = \"post\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print (encoded_sent)\n",
    "# print (\"Num of Sentences: \", len(encoded_sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "colab_type": "code",
    "id": "gDgTCS2KdI2p",
    "outputId": "ac5332cd-0a0f-4311-8db4-22df92728d90"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  4,   1,  23, 129,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "       [  2,   4,  60,  61,  24,   0,   0,   0,   0,   0,   0,   0],\n",
       "       [ 89,  31,  21,  15,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "       [  7,  10,  24,  73,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "       [  4,   1,  23,  46,   0,   0,   0,   0,   0,   0,   0,   0]],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_sent[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "3eaSIDi0dNf1",
    "outputId": "4ab6b6dd-ffa4-4061-9e9d-7a01decfa837"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of padded sent =  (346, 12)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of padded sent = \",padded_sent.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizing the intents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X0rXzenSpgFR"
   },
   "outputs": [],
   "source": [
    "#tokenizer for the intents\n",
    "token_intents = Tokenizer(filters = '!\"#$%&()*+,-/:;<=>?@[\\]^`{|}~')\n",
    "token_intents.fit_on_texts(intents_unique)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "colab_type": "code",
    "id": "yNHQtkszskxr",
    "outputId": "f5babc01-89e3-4392-e8e6-c9f257de3d07"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'leave_budget': 1,\n",
       " 'deny': 2,\n",
       " 'goodbye': 3,\n",
       " 'thanks': 4,\n",
       " 'confirm': 5,\n",
       " 'sorry': 6,\n",
       " 'what_is_your_name': 7,\n",
       " 'are_you_a_robot': 8,\n",
       " 'insult': 9,\n",
       " 'how_are_you': 10,\n",
       " 'greet': 11,\n",
       " 'conversation_restart': 12,\n",
       " 'skills': 13,\n",
       " 'personal_question': 14}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_intents.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7OOx9qdBto1-"
   },
   "outputs": [],
   "source": [
    "encoded_output = token_intents.texts_to_sequences(intents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print (encoded_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0_5Lv5PiyG-z"
   },
   "outputs": [],
   "source": [
    "encoded_output = np.array(encoded_output).reshape(len(encoded_output), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print (encoded_output)\n",
    "type (encoded_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "dpM86WrVQlx5",
    "outputId": "71ff52a6-b3d0-4b5c-850d-5dc0a56c8aa9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(346, 1)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rD3QN-RPzfet"
   },
   "outputs": [],
   "source": [
    "one_hot = OneHotEncoder(sparse = False)\n",
    "output_one_hot = one_hot.fit_transform(encoded_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "A6HVslLTHgOM",
    "outputId": "752962df-02d8-409b-fb8f-adb06227161d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(346, 14)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_one_hot.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Training- and Testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EqABUESD7xi9"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h8P4HTz6A4E-"
   },
   "outputs": [],
   "source": [
    "# train_X, val_X, train_Y, val_Y = train_test_split(padded_sent, output_one_hot, shuffle = True, test_size = 0.1)\n",
    "train_X = padded_sent\n",
    "train_Y = output_one_hot\n",
    "val_X   = train_X\n",
    "val_Y   = train_Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "7E0uhC2OCtTx",
    "outputId": "6ce0e215-aa3f-43f1-ba5a-0b584b25a35c",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train_X = (346, 12) and train_Y = (346, 14)\n",
      "Shape of val_X = (346, 12) and val_Y = (346, 14)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of train_X = %s and train_Y = %s\" % (train_X.shape, train_Y.shape))\n",
    "print(\"Shape of val_X = %s and val_Y = %s\" % (val_X.shape, val_Y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Embeddings (from glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400001 word vectors.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "GLOVE_DIR  = \"/Users/cwindheu/gensim-data/glove-wiki-gigaword-200/\"\n",
    "GLOVE_FILE = \"glove-wiki-gigaword-200.txt\"\n",
    "EMBEDDING_DIM = 200\n",
    "\n",
    "embeddings_index = {}\n",
    "\n",
    "f = open(os.path.join(GLOVE_DIR, GLOVE_FILE))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not in vacabulary:  chatbot\n",
      "Not in vacabulary:  amayzing\n",
      "Not in vacabulary:  heeey\n",
      "Not in vacabulary:  helloooo\n",
      "Not in vacabulary:  jojojo\n",
      "Not in vacabulary:  thanx\n",
      "Not in vacabulary:  hellllooooooo\n",
      "Not in vacabulary:  hellooo\n",
      "Not in vacabulary:  hiihihi\n",
      "Not in vacabulary:  thnx\n",
      "Not in vacabulary:  heyho\n",
      "Not in vacabulary:  hiii\n",
      "Not in vacabulary:  sweatheart\n",
      "Not in vacabulary:  heyo\n",
      "Not in vacabulary:  ayyyy\n",
      "Not in vacabulary:  whaddup\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_DIM = 200\n",
    "\n",
    "embedding_matrix = np.zeros((VOCABULARY_SIZE, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    else:\n",
    "        print (\"Not in vacabulary: \", word)\n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e5BU_x74DNEb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_7 (Embedding)      (None, 12, 200)           56800     \n",
      "_________________________________________________________________\n",
      "bidirectional_7 (Bidirection (None, 256)               336896    \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 32)                8224      \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 14)                462       \n",
      "=================================================================\n",
      "Total params: 402,382\n",
      "Trainable params: 402,382\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(VOCABULARY_SIZE, EMBEDDING_DIM, weights=[embedding_matrix], input_length = MAX_SENT_LENGTH, trainable = True))\n",
    "model.add(Bidirectional(LSTM(128)))\n",
    "model.add(Dense(32, activation = \"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(NUM_INTENTS_UNIQUE, activation = \"softmax\"))\n",
    "\n",
    "model.compile(loss = \"categorical_crossentropy\", optimizer = \"adam\", metrics = [\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 6834
    },
    "colab_type": "code",
    "id": "_r-dxm2sMQ-d",
    "outputId": "3c37b4f8-fc4e-4c82-ab46-2aa1d8b47ffd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 346 samples, validate on 346 samples\n",
      "Epoch 1/200\n",
      "346/346 [==============================] - 6s 16ms/step - loss: 2.5585 - acc: 0.1936 - val_loss: 2.3180 - val_acc: 0.4104\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 2.31797, saving model to dir_01.h5\n",
      "Epoch 2/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 2.2520 - acc: 0.3988 - val_loss: 2.0292 - val_acc: 0.4884\n",
      "\n",
      "Epoch 00002: val_loss improved from 2.31797 to 2.02916, saving model to dir_01.h5\n",
      "Epoch 3/200\n",
      "346/346 [==============================] - 1s 2ms/step - loss: 1.9884 - acc: 0.4306 - val_loss: 1.6625 - val_acc: 0.5491\n",
      "\n",
      "Epoch 00003: val_loss improved from 2.02916 to 1.66250, saving model to dir_01.h5\n",
      "Epoch 4/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 1.6384 - acc: 0.5318 - val_loss: 1.3468 - val_acc: 0.6185\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.66250 to 1.34680, saving model to dir_01.h5\n",
      "Epoch 5/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 1.5490 - acc: 0.5318 - val_loss: 1.1346 - val_acc: 0.6850\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.34680 to 1.13456, saving model to dir_01.h5\n",
      "Epoch 6/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 1.3286 - acc: 0.5694 - val_loss: 0.9496 - val_acc: 0.7197\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.13456 to 0.94963, saving model to dir_01.h5\n",
      "Epoch 7/200\n",
      "346/346 [==============================] - 1s 2ms/step - loss: 1.0736 - acc: 0.6416 - val_loss: 0.7360 - val_acc: 0.7514\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.94963 to 0.73597, saving model to dir_01.h5\n",
      "Epoch 8/200\n",
      "346/346 [==============================] - 1s 2ms/step - loss: 0.9907 - acc: 0.6936 - val_loss: 0.6165 - val_acc: 0.8353\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.73597 to 0.61654, saving model to dir_01.h5\n",
      "Epoch 9/200\n",
      "346/346 [==============================] - 1s 2ms/step - loss: 0.8501 - acc: 0.7428 - val_loss: 0.5677 - val_acc: 0.8844\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.61654 to 0.56770, saving model to dir_01.h5\n",
      "Epoch 10/200\n",
      "346/346 [==============================] - 1s 2ms/step - loss: 0.7216 - acc: 0.7688 - val_loss: 0.3817 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.56770 to 0.38167, saving model to dir_01.h5\n",
      "Epoch 11/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.6179 - acc: 0.7977 - val_loss: 0.2888 - val_acc: 0.9335\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.38167 to 0.28883, saving model to dir_01.h5\n",
      "Epoch 12/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.4595 - acc: 0.8728 - val_loss: 0.2200 - val_acc: 0.9538\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.28883 to 0.21998, saving model to dir_01.h5\n",
      "Epoch 13/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.4184 - acc: 0.8555 - val_loss: 0.1553 - val_acc: 0.9711\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.21998 to 0.15531, saving model to dir_01.h5\n",
      "Epoch 14/200\n",
      "346/346 [==============================] - 0s 997us/step - loss: 0.3848 - acc: 0.8786 - val_loss: 0.1473 - val_acc: 0.9769\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.15531 to 0.14726, saving model to dir_01.h5\n",
      "Epoch 15/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.3161 - acc: 0.9046 - val_loss: 0.1139 - val_acc: 0.9827\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.14726 to 0.11392, saving model to dir_01.h5\n",
      "Epoch 16/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.3176 - acc: 0.8902 - val_loss: 0.1062 - val_acc: 0.9827\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.11392 to 0.10623, saving model to dir_01.h5\n",
      "Epoch 17/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.2816 - acc: 0.9364 - val_loss: 0.0808 - val_acc: 0.9913\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.10623 to 0.08076, saving model to dir_01.h5\n",
      "Epoch 18/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.2383 - acc: 0.9249 - val_loss: 0.0597 - val_acc: 0.9942\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.08076 to 0.05975, saving model to dir_01.h5\n",
      "Epoch 19/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.2662 - acc: 0.9306 - val_loss: 0.0528 - val_acc: 0.9913\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.05975 to 0.05277, saving model to dir_01.h5\n",
      "Epoch 20/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.2055 - acc: 0.9480 - val_loss: 0.0530 - val_acc: 0.9942\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.05277\n",
      "Epoch 21/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.2611 - acc: 0.9335 - val_loss: 0.0556 - val_acc: 0.9884\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.05277\n",
      "Epoch 22/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.1897 - acc: 0.9480 - val_loss: 0.0317 - val_acc: 0.9971\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.05277 to 0.03171, saving model to dir_01.h5\n",
      "Epoch 23/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.1962 - acc: 0.9364 - val_loss: 0.0274 - val_acc: 0.9971\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.03171 to 0.02741, saving model to dir_01.h5\n",
      "Epoch 24/200\n",
      "346/346 [==============================] - 1s 2ms/step - loss: 0.1587 - acc: 0.9480 - val_loss: 0.0239 - val_acc: 0.9971\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.02741 to 0.02389, saving model to dir_01.h5\n",
      "Epoch 25/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.1321 - acc: 0.9682 - val_loss: 0.0214 - val_acc: 0.9942\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.02389 to 0.02141, saving model to dir_01.h5\n",
      "Epoch 26/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.1752 - acc: 0.9306 - val_loss: 0.0377 - val_acc: 0.9855\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.02141\n",
      "Epoch 27/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.2167 - acc: 0.9191 - val_loss: 0.0416 - val_acc: 0.9942\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.02141\n",
      "Epoch 28/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.1491 - acc: 0.9653 - val_loss: 0.0258 - val_acc: 0.9942\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.02141\n",
      "Epoch 29/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.1355 - acc: 0.9653 - val_loss: 0.0193 - val_acc: 0.9942\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.02141 to 0.01929, saving model to dir_01.h5\n",
      "Epoch 30/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.1344 - acc: 0.9566 - val_loss: 0.0209 - val_acc: 0.9942\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.01929\n",
      "Epoch 31/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.1514 - acc: 0.9509 - val_loss: 0.0219 - val_acc: 0.9942\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.01929\n",
      "Epoch 32/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.1371 - acc: 0.9653 - val_loss: 0.0161 - val_acc: 0.9971\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.01929 to 0.01608, saving model to dir_01.h5\n",
      "Epoch 33/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.1398 - acc: 0.9538 - val_loss: 0.0119 - val_acc: 0.9942\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.01608 to 0.01186, saving model to dir_01.h5\n",
      "Epoch 34/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.1177 - acc: 0.9566 - val_loss: 0.0101 - val_acc: 0.9971\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.01186 to 0.01007, saving model to dir_01.h5\n",
      "Epoch 35/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.0887 - acc: 0.9711 - val_loss: 0.0089 - val_acc: 0.9971\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.01007 to 0.00889, saving model to dir_01.h5\n",
      "Epoch 36/200\n",
      "346/346 [==============================] - 1s 2ms/step - loss: 0.1342 - acc: 0.9624 - val_loss: 0.0094 - val_acc: 0.9971\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.00889\n",
      "Epoch 37/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.1524 - acc: 0.9480 - val_loss: 0.0260 - val_acc: 0.9884\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.00889\n",
      "Epoch 38/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.0925 - acc: 0.9682 - val_loss: 0.0118 - val_acc: 0.9971\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.00889\n",
      "Epoch 39/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.1226 - acc: 0.9595 - val_loss: 0.0300 - val_acc: 0.9942\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.00889\n",
      "Epoch 40/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "346/346 [==============================] - 0s 1ms/step - loss: 0.1777 - acc: 0.9480 - val_loss: 0.0201 - val_acc: 0.9942\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.00889\n",
      "Epoch 41/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.1298 - acc: 0.9682 - val_loss: 0.0109 - val_acc: 0.9971\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.00889\n",
      "Epoch 42/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.0977 - acc: 0.9740 - val_loss: 0.0113 - val_acc: 0.9971\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.00889\n",
      "Epoch 43/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.1236 - acc: 0.9595 - val_loss: 0.0295 - val_acc: 0.9884\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.00889\n",
      "Epoch 44/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.1726 - acc: 0.9364 - val_loss: 0.0117 - val_acc: 0.9971\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.00889\n",
      "Epoch 45/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.0814 - acc: 0.9827 - val_loss: 0.0104 - val_acc: 0.9971\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.00889\n",
      "Epoch 46/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.1324 - acc: 0.9624 - val_loss: 0.0085 - val_acc: 0.9971\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.00889 to 0.00852, saving model to dir_01.h5\n",
      "Epoch 47/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.0801 - acc: 0.9769 - val_loss: 0.0072 - val_acc: 0.9971\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.00852 to 0.00719, saving model to dir_01.h5\n",
      "Epoch 48/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.1020 - acc: 0.9711 - val_loss: 0.0091 - val_acc: 0.9971\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.00719\n",
      "Epoch 49/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.0671 - acc: 0.9798 - val_loss: 0.0072 - val_acc: 0.9971\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.00719 to 0.00716, saving model to dir_01.h5\n",
      "Epoch 50/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.0825 - acc: 0.9711 - val_loss: 0.0066 - val_acc: 0.9971\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.00716 to 0.00659, saving model to dir_01.h5\n",
      "Epoch 51/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.0970 - acc: 0.9682 - val_loss: 0.0058 - val_acc: 0.9971\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.00659 to 0.00585, saving model to dir_01.h5\n",
      "Epoch 52/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.0720 - acc: 0.9740 - val_loss: 0.0060 - val_acc: 0.9971\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.00585\n",
      "Epoch 53/200\n",
      "346/346 [==============================] - 1s 2ms/step - loss: 0.0856 - acc: 0.9740 - val_loss: 0.0053 - val_acc: 0.9971\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.00585 to 0.00533, saving model to dir_01.h5\n",
      "Epoch 54/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.0809 - acc: 0.9769 - val_loss: 0.0050 - val_acc: 0.9971\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.00533 to 0.00502, saving model to dir_01.h5\n",
      "Epoch 55/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.0640 - acc: 0.9769 - val_loss: 0.0056 - val_acc: 0.9971\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.00502\n",
      "Epoch 56/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.0733 - acc: 0.9711 - val_loss: 0.0052 - val_acc: 0.9971\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.00502\n",
      "Epoch 57/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.1063 - acc: 0.9566 - val_loss: 0.0057 - val_acc: 0.9971\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.00502\n",
      "Epoch 58/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.0747 - acc: 0.9827 - val_loss: 0.0055 - val_acc: 0.9971\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.00502\n",
      "Epoch 59/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.0621 - acc: 0.9682 - val_loss: 0.0051 - val_acc: 0.9971\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.00502\n",
      "Epoch 60/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.0392 - acc: 0.9913 - val_loss: 0.0053 - val_acc: 0.9971\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.00502\n",
      "Epoch 61/200\n",
      "346/346 [==============================] - 1s 2ms/step - loss: 0.0597 - acc: 0.9798 - val_loss: 0.0047 - val_acc: 0.9971\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.00502 to 0.00465, saving model to dir_01.h5\n",
      "Epoch 62/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.0448 - acc: 0.9855 - val_loss: 0.0046 - val_acc: 0.9971\n",
      "\n",
      "Epoch 00062: val_loss improved from 0.00465 to 0.00462, saving model to dir_01.h5\n",
      "Epoch 63/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.0713 - acc: 0.9682 - val_loss: 0.0049 - val_acc: 0.9971\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.00462\n",
      "Epoch 64/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.0541 - acc: 0.9798 - val_loss: 0.0048 - val_acc: 0.9971\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.00462\n",
      "Epoch 65/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.0602 - acc: 0.9798 - val_loss: 0.0049 - val_acc: 0.9971\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.00462\n",
      "Epoch 66/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.0522 - acc: 0.9855 - val_loss: 0.0049 - val_acc: 0.9971\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.00462\n",
      "Epoch 67/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.0785 - acc: 0.9653 - val_loss: 0.0047 - val_acc: 0.9971\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.00462\n",
      "Epoch 68/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.0758 - acc: 0.9769 - val_loss: 0.0051 - val_acc: 0.9971\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.00462\n",
      "Epoch 69/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.0590 - acc: 0.9855 - val_loss: 0.0049 - val_acc: 0.9971\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.00462\n",
      "Epoch 70/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.0559 - acc: 0.9827 - val_loss: 0.0048 - val_acc: 0.9971\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.00462\n",
      "Epoch 71/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.0520 - acc: 0.9769 - val_loss: 0.0046 - val_acc: 0.9971\n",
      "\n",
      "Epoch 00071: val_loss improved from 0.00462 to 0.00461, saving model to dir_01.h5\n",
      "Epoch 72/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.0614 - acc: 0.9855 - val_loss: 0.0044 - val_acc: 0.9971\n",
      "\n",
      "Epoch 00072: val_loss improved from 0.00461 to 0.00443, saving model to dir_01.h5\n",
      "Epoch 73/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.0435 - acc: 0.9855 - val_loss: 0.0044 - val_acc: 0.9971\n",
      "\n",
      "Epoch 00073: val_loss improved from 0.00443 to 0.00436, saving model to dir_01.h5\n",
      "Epoch 74/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.0487 - acc: 0.9740 - val_loss: 0.0043 - val_acc: 0.9971\n",
      "\n",
      "Epoch 00074: val_loss improved from 0.00436 to 0.00430, saving model to dir_01.h5\n",
      "Epoch 75/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.0607 - acc: 0.9711 - val_loss: 0.0046 - val_acc: 0.9971\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.00430\n",
      "Epoch 76/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.0564 - acc: 0.9682 - val_loss: 0.0046 - val_acc: 0.9971\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.00430\n",
      "Epoch 77/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.0440 - acc: 0.9913 - val_loss: 0.0046 - val_acc: 0.9971\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.00430\n",
      "Epoch 78/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.0579 - acc: 0.9711 - val_loss: 0.0044 - val_acc: 0.9971\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.00430\n",
      "Epoch 79/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.0508 - acc: 0.9827 - val_loss: 0.0043 - val_acc: 0.9971\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.00430\n",
      "Epoch 80/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.0618 - acc: 0.9653 - val_loss: 0.0043 - val_acc: 0.9971\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.00430\n",
      "Epoch 81/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.0676 - acc: 0.9711 - val_loss: 0.0043 - val_acc: 0.9971\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.00430\n",
      "Epoch 82/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "346/346 [==============================] - 0s 1ms/step - loss: 0.0473 - acc: 0.9855 - val_loss: 0.0045 - val_acc: 0.9971\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.00430\n",
      "Epoch 83/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.0715 - acc: 0.9624 - val_loss: 0.0057 - val_acc: 0.9971\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.00430\n",
      "Epoch 84/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.0828 - acc: 0.9798 - val_loss: 0.0053 - val_acc: 0.9971\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.00430\n",
      "Epoch 85/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.0406 - acc: 0.9884 - val_loss: 0.0049 - val_acc: 0.9971\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.00430\n",
      "Epoch 86/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.0804 - acc: 0.9740 - val_loss: 0.0046 - val_acc: 0.9971\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.00430\n",
      "Epoch 87/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.0584 - acc: 0.9769 - val_loss: 0.0046 - val_acc: 0.9971\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.00430\n",
      "Epoch 88/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.0591 - acc: 0.9827 - val_loss: 0.0044 - val_acc: 0.9971\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.00430\n",
      "Epoch 89/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.0750 - acc: 0.9653 - val_loss: 0.0043 - val_acc: 0.9971\n",
      "\n",
      "Epoch 00089: val_loss improved from 0.00430 to 0.00425, saving model to dir_01.h5\n",
      "Epoch 90/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.0475 - acc: 0.9827 - val_loss: 0.0042 - val_acc: 0.9971\n",
      "\n",
      "Epoch 00090: val_loss improved from 0.00425 to 0.00421, saving model to dir_01.h5\n",
      "Epoch 91/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.0641 - acc: 0.9740 - val_loss: 0.0042 - val_acc: 0.9971\n",
      "\n",
      "Epoch 00091: val_loss improved from 0.00421 to 0.00415, saving model to dir_01.h5\n",
      "Epoch 92/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.0268 - acc: 0.9913 - val_loss: 0.0041 - val_acc: 0.9971\n",
      "\n",
      "Epoch 00092: val_loss improved from 0.00415 to 0.00413, saving model to dir_01.h5\n",
      "Epoch 93/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.0498 - acc: 0.9798 - val_loss: 0.0057 - val_acc: 0.9971\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.00413\n",
      "Epoch 94/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.0416 - acc: 0.9855 - val_loss: 0.0041 - val_acc: 0.9971\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.00413\n",
      "Epoch 95/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.0515 - acc: 0.9855 - val_loss: 0.0041 - val_acc: 0.9971\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.00413\n",
      "Epoch 96/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.0539 - acc: 0.9769 - val_loss: 0.0049 - val_acc: 0.9971\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.00413\n",
      "Epoch 97/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.0346 - acc: 0.9855 - val_loss: 0.0047 - val_acc: 0.9971\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.00413\n",
      "Epoch 98/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.0731 - acc: 0.9682 - val_loss: 0.0043 - val_acc: 0.9971\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.00413\n",
      "Epoch 99/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.0375 - acc: 0.9884 - val_loss: 0.0043 - val_acc: 0.9971\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.00413\n",
      "Epoch 100/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.0527 - acc: 0.9827 - val_loss: 0.0042 - val_acc: 0.9971\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.00413\n",
      "Epoch 101/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.0696 - acc: 0.9740 - val_loss: 0.0045 - val_acc: 0.9971\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 0.00413\n",
      "Epoch 102/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.0438 - acc: 0.9855 - val_loss: 0.0045 - val_acc: 0.9971\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 0.00413\n",
      "Epoch 103/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.0631 - acc: 0.9769 - val_loss: 0.0042 - val_acc: 0.9971\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 0.00413\n",
      "Epoch 104/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.0425 - acc: 0.9740 - val_loss: 0.0042 - val_acc: 0.9971\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 0.00413\n",
      "Epoch 105/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.0589 - acc: 0.9798 - val_loss: 0.0042 - val_acc: 0.9971\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 0.00413\n",
      "Epoch 106/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.0308 - acc: 0.9942 - val_loss: 0.0042 - val_acc: 0.9971\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 0.00413\n",
      "Epoch 107/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.0523 - acc: 0.9798 - val_loss: 0.0073 - val_acc: 0.9942\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 0.00413\n",
      "Epoch 108/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.0413 - acc: 0.9769 - val_loss: 0.0042 - val_acc: 0.9971\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 0.00413\n",
      "Epoch 109/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.0709 - acc: 0.9769 - val_loss: 0.0044 - val_acc: 0.9971\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 0.00413\n",
      "Epoch 110/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.0580 - acc: 0.9740 - val_loss: 0.0045 - val_acc: 0.9971\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 0.00413\n",
      "Epoch 111/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.0681 - acc: 0.9769 - val_loss: 0.0042 - val_acc: 0.9971\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 0.00413\n",
      "Epoch 112/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.0625 - acc: 0.9740 - val_loss: 0.0042 - val_acc: 0.9971\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 0.00413\n",
      "Epoch 113/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.0493 - acc: 0.9798 - val_loss: 0.0043 - val_acc: 0.9971\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 0.00413\n",
      "Epoch 114/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.0512 - acc: 0.9827 - val_loss: 0.0045 - val_acc: 0.9971\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 0.00413\n",
      "Epoch 115/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.0648 - acc: 0.9740 - val_loss: 0.0047 - val_acc: 0.9971\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 0.00413\n",
      "Epoch 116/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.0471 - acc: 0.9798 - val_loss: 0.0046 - val_acc: 0.9971\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 0.00413\n",
      "Epoch 117/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.0621 - acc: 0.9711 - val_loss: 0.0043 - val_acc: 0.9971\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 0.00413\n",
      "Epoch 118/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.0381 - acc: 0.9913 - val_loss: 0.0042 - val_acc: 0.9971\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 0.00413\n",
      "Epoch 119/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.0409 - acc: 0.9827 - val_loss: 0.0042 - val_acc: 0.9971\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 0.00413\n",
      "Epoch 120/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.0372 - acc: 0.9884 - val_loss: 0.0042 - val_acc: 0.9971\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 0.00413\n",
      "Epoch 121/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.0420 - acc: 0.9798 - val_loss: 0.0041 - val_acc: 0.9971\n",
      "\n",
      "Epoch 00121: val_loss improved from 0.00413 to 0.00413, saving model to dir_01.h5\n",
      "Epoch 122/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.0333 - acc: 0.9884 - val_loss: 0.0041 - val_acc: 0.9971\n",
      "\n",
      "Epoch 00122: val_loss improved from 0.00413 to 0.00410, saving model to dir_01.h5\n",
      "Epoch 123/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.0387 - acc: 0.9769 - val_loss: 0.0042 - val_acc: 0.9971\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 0.00410\n",
      "Epoch 124/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.0423 - acc: 0.9798 - val_loss: 0.0042 - val_acc: 0.9971\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 0.00410\n",
      "Epoch 125/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "346/346 [==============================] - 0s 1ms/step - loss: 0.0516 - acc: 0.9769 - val_loss: 0.0042 - val_acc: 0.9971\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 0.00410\n",
      "Epoch 126/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.0524 - acc: 0.9769 - val_loss: 0.0042 - val_acc: 0.9971\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 0.00410\n",
      "Epoch 127/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.0365 - acc: 0.9855 - val_loss: 0.0042 - val_acc: 0.9971\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 0.00410\n",
      "Epoch 128/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.0630 - acc: 0.9769 - val_loss: 0.0042 - val_acc: 0.9971\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 0.00410\n",
      "Epoch 129/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.0308 - acc: 0.9855 - val_loss: 0.0042 - val_acc: 0.9971\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 0.00410\n",
      "Epoch 130/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.0416 - acc: 0.9855 - val_loss: 0.0043 - val_acc: 0.9971\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 0.00410\n",
      "Epoch 131/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.0408 - acc: 0.9798 - val_loss: 0.0042 - val_acc: 0.9971\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 0.00410\n",
      "Epoch 132/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.0519 - acc: 0.9827 - val_loss: 0.0041 - val_acc: 0.9971\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 0.00410\n",
      "Epoch 133/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.0536 - acc: 0.9798 - val_loss: 0.0041 - val_acc: 0.9971\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 0.00410\n",
      "Epoch 134/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.0477 - acc: 0.9769 - val_loss: 0.0041 - val_acc: 0.9971\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 0.00410\n",
      "Epoch 135/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.0660 - acc: 0.9682 - val_loss: 0.0041 - val_acc: 0.9971\n",
      "\n",
      "Epoch 00135: val_loss improved from 0.00410 to 0.00410, saving model to dir_01.h5\n",
      "Epoch 136/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.0598 - acc: 0.9798 - val_loss: 0.0041 - val_acc: 0.9971\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 0.00410\n",
      "Epoch 137/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.0329 - acc: 0.9855 - val_loss: 0.0041 - val_acc: 0.9971\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 0.00410\n",
      "Epoch 138/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.0482 - acc: 0.9827 - val_loss: 0.0041 - val_acc: 0.9971\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 0.00410\n",
      "Epoch 139/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.0466 - acc: 0.9798 - val_loss: 0.0041 - val_acc: 0.9971\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 0.00410\n",
      "Epoch 140/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.0379 - acc: 0.9798 - val_loss: 0.0044 - val_acc: 0.9971\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 0.00410\n",
      "Epoch 141/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.0419 - acc: 0.9769 - val_loss: 0.0041 - val_acc: 0.9971\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 0.00410\n",
      "Epoch 142/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.0414 - acc: 0.9827 - val_loss: 0.0042 - val_acc: 0.9971\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 0.00410\n",
      "Epoch 143/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.0462 - acc: 0.9740 - val_loss: 0.0041 - val_acc: 0.9971\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 0.00410\n",
      "Epoch 144/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.0462 - acc: 0.9827 - val_loss: 0.0041 - val_acc: 0.9971\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 0.00410\n",
      "Epoch 145/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.0440 - acc: 0.9769 - val_loss: 0.0042 - val_acc: 0.9971\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 0.00410\n",
      "Epoch 146/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.0373 - acc: 0.9827 - val_loss: 0.0044 - val_acc: 0.9971\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 0.00410\n",
      "Epoch 147/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.0806 - acc: 0.9682 - val_loss: 0.0152 - val_acc: 0.9942\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 0.00410\n",
      "Epoch 148/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.2322 - acc: 0.9451 - val_loss: 0.5196 - val_acc: 0.9133\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 0.00410\n",
      "Epoch 149/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.3433 - acc: 0.8931 - val_loss: 0.0843 - val_acc: 0.9682\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 0.00410\n",
      "Epoch 150/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.1621 - acc: 0.9624 - val_loss: 0.0167 - val_acc: 0.9971\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 0.00410\n",
      "Epoch 151/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.0658 - acc: 0.9855 - val_loss: 0.0085 - val_acc: 0.9971\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 0.00410\n",
      "Epoch 152/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.0774 - acc: 0.9740 - val_loss: 0.0052 - val_acc: 0.9971\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 0.00410\n",
      "Epoch 153/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.0555 - acc: 0.9769 - val_loss: 0.0085 - val_acc: 0.9971\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 0.00410\n",
      "Epoch 154/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.0801 - acc: 0.9682 - val_loss: 0.0052 - val_acc: 0.9971\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 0.00410\n",
      "Epoch 155/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.0775 - acc: 0.9711 - val_loss: 0.0046 - val_acc: 0.9971\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 0.00410\n",
      "Epoch 156/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.0282 - acc: 0.9913 - val_loss: 0.0044 - val_acc: 0.9971\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 0.00410\n",
      "Epoch 157/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.0489 - acc: 0.9855 - val_loss: 0.0043 - val_acc: 0.9971\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 0.00410\n",
      "Epoch 158/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.0300 - acc: 0.9884 - val_loss: 0.0044 - val_acc: 0.9971\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 0.00410\n",
      "Epoch 159/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.0521 - acc: 0.9769 - val_loss: 0.0043 - val_acc: 0.9971\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 0.00410\n",
      "Epoch 160/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.0351 - acc: 0.9942 - val_loss: 0.0045 - val_acc: 0.9971\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 0.00410\n",
      "Epoch 161/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.0422 - acc: 0.9855 - val_loss: 0.0042 - val_acc: 0.9971\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 0.00410\n",
      "Epoch 162/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.0452 - acc: 0.9827 - val_loss: 0.0042 - val_acc: 0.9971\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 0.00410\n",
      "Epoch 163/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.0578 - acc: 0.9740 - val_loss: 0.0041 - val_acc: 0.9971\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 0.00410\n",
      "Epoch 164/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.0188 - acc: 0.9942 - val_loss: 0.0041 - val_acc: 0.9971\n",
      "\n",
      "Epoch 00164: val_loss improved from 0.00410 to 0.00410, saving model to dir_01.h5\n",
      "Epoch 165/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.0306 - acc: 0.9942 - val_loss: 0.0041 - val_acc: 0.9971\n",
      "\n",
      "Epoch 00165: val_loss improved from 0.00410 to 0.00408, saving model to dir_01.h5\n",
      "Epoch 166/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.0409 - acc: 0.9769 - val_loss: 0.0041 - val_acc: 0.9971\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 0.00408\n",
      "Epoch 167/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.0407 - acc: 0.9884 - val_loss: 0.0041 - val_acc: 0.9971\n",
      "\n",
      "Epoch 00167: val_loss improved from 0.00408 to 0.00405, saving model to dir_01.h5\n",
      "Epoch 168/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "346/346 [==============================] - 0s 1ms/step - loss: 0.0590 - acc: 0.9769 - val_loss: 0.0041 - val_acc: 0.9971\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 0.00405\n",
      "Epoch 169/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.0385 - acc: 0.9855 - val_loss: 0.0042 - val_acc: 0.9971\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 0.00405\n",
      "Epoch 170/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.0276 - acc: 0.9884 - val_loss: 0.0041 - val_acc: 0.9971\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 0.00405\n",
      "Epoch 171/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.0349 - acc: 0.9855 - val_loss: 0.0041 - val_acc: 0.9971\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 0.00405\n",
      "Epoch 172/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.0315 - acc: 0.9884 - val_loss: 0.0041 - val_acc: 0.9971\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 0.00405\n",
      "Epoch 173/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.0373 - acc: 0.9855 - val_loss: 0.0041 - val_acc: 0.9971\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 0.00405\n",
      "Epoch 174/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.0395 - acc: 0.9769 - val_loss: 0.0042 - val_acc: 0.9971\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 0.00405\n",
      "Epoch 175/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.0546 - acc: 0.9827 - val_loss: 0.0134 - val_acc: 0.9942\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 0.00405\n",
      "Epoch 176/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.2051 - acc: 0.9538 - val_loss: 0.0712 - val_acc: 0.9855\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 0.00405\n",
      "Epoch 177/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.2036 - acc: 0.9480 - val_loss: 0.0911 - val_acc: 0.9884\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 0.00405\n",
      "Epoch 178/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.1466 - acc: 0.9711 - val_loss: 0.0326 - val_acc: 0.9913\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 0.00405\n",
      "Epoch 179/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.0639 - acc: 0.9827 - val_loss: 0.0272 - val_acc: 0.9913\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 0.00405\n",
      "Epoch 180/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.1066 - acc: 0.9595 - val_loss: 0.0049 - val_acc: 0.9971\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 0.00405\n",
      "Epoch 181/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.1158 - acc: 0.9595 - val_loss: 0.0121 - val_acc: 0.9942\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 0.00405\n",
      "Epoch 182/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.1436 - acc: 0.9451 - val_loss: 0.0113 - val_acc: 0.9942\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 0.00405\n",
      "Epoch 183/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.0867 - acc: 0.9769 - val_loss: 0.0057 - val_acc: 0.9971\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 0.00405\n",
      "Epoch 184/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.0710 - acc: 0.9682 - val_loss: 0.0044 - val_acc: 0.9971\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 0.00405\n",
      "Epoch 185/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.0687 - acc: 0.9855 - val_loss: 0.0042 - val_acc: 0.9971\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 0.00405\n",
      "Epoch 186/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.0527 - acc: 0.9711 - val_loss: 0.0041 - val_acc: 0.9971\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 0.00405\n",
      "Epoch 187/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.0493 - acc: 0.9711 - val_loss: 0.0041 - val_acc: 0.9971\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 0.00405\n",
      "Epoch 188/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.0589 - acc: 0.9769 - val_loss: 0.0041 - val_acc: 0.9971\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 0.00405\n",
      "Epoch 189/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.0477 - acc: 0.9798 - val_loss: 0.0041 - val_acc: 0.9971\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 0.00405\n",
      "Epoch 190/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.0680 - acc: 0.9740 - val_loss: 0.0041 - val_acc: 0.9971\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 0.00405\n",
      "Epoch 191/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.0308 - acc: 0.9884 - val_loss: 0.0041 - val_acc: 0.9971\n",
      "\n",
      "Epoch 00191: val_loss improved from 0.00405 to 0.00405, saving model to dir_01.h5\n",
      "Epoch 192/200\n",
      "346/346 [==============================] - ETA: 0s - loss: 0.0420 - acc: 0.975 - 0s 1ms/step - loss: 0.0417 - acc: 0.9740 - val_loss: 0.0041 - val_acc: 0.9971\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 0.00405\n",
      "Epoch 193/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.0542 - acc: 0.9798 - val_loss: 0.0041 - val_acc: 0.9971\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 0.00405\n",
      "Epoch 194/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.0520 - acc: 0.9798 - val_loss: 0.0041 - val_acc: 0.9971\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 0.00405\n",
      "Epoch 195/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.0322 - acc: 0.9884 - val_loss: 0.0041 - val_acc: 0.9971\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 0.00405\n",
      "Epoch 196/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.0369 - acc: 0.9913 - val_loss: 0.0042 - val_acc: 0.9971\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 0.00405\n",
      "Epoch 197/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.0394 - acc: 0.9913 - val_loss: 0.0043 - val_acc: 0.9971\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 0.00405\n",
      "Epoch 198/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.0311 - acc: 0.9855 - val_loss: 0.0049 - val_acc: 0.9971\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 0.00405\n",
      "Epoch 199/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.0493 - acc: 0.9798 - val_loss: 0.0045 - val_acc: 0.9971\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 0.00405\n",
      "Epoch 200/200\n",
      "346/346 [==============================] - 0s 1ms/step - loss: 0.0400 - acc: 0.9855 - val_loss: 0.0041 - val_acc: 0.9971\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 0.00405\n",
      "Elapsed time in seconds:  100.31665897369385\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "filename = 'dir_01.h5'\n",
    "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "hist = model.fit(train_X, train_Y, epochs = 200, batch_size = 32, validation_data = (val_X, val_Y), callbacks = [checkpoint])\n",
    "\n",
    "print(\"Elapsed time in seconds: \", time.time() - start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YjXKos8ocXvw"
   },
   "outputs": [],
   "source": [
    " model = load_model(\"dir_01.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qSTEzrlzcuya"
   },
   "outputs": [],
   "source": [
    "def predictions(text):\n",
    "    clean = re.sub(r'[^ a-z A-Z 0-9]', \" \", text)\n",
    "    test_word = word_tokenize(clean)\n",
    "    test_word = [w.lower() for w in test_word]\n",
    "    test_ls = token.texts_to_sequences(test_word)\n",
    "    #print(test_word)\n",
    "    #Check for unknown words\n",
    "    if [] in test_ls:\n",
    "        test_ls = list(filter(None, test_ls))\n",
    "    \n",
    "    test_ls = np.array(test_ls).reshape(1, len(test_ls))\n",
    "\n",
    "    #print(\"test_ls: \", test_ls)\n",
    "\n",
    "    x = pad_sequences(test_ls, maxlen = MAX_SENT_LENGTH, padding = \"post\")\n",
    "    \n",
    "    # print (\"x: \", x)\n",
    "    \n",
    "    pred = model.predict_proba(x)\n",
    "  \n",
    "    return pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P1ddofshmdzK"
   },
   "outputs": [],
   "source": [
    "def get_final_output(pred, classes):\n",
    "    #print (type (pred))\n",
    "    #print (pred)\n",
    "    \n",
    "    predictions = pred[0]\n",
    " \n",
    "    classes = np.array(classes)\n",
    "    ids = np.argsort(-predictions)\n",
    "    classes = classes[ids]\n",
    "#    predictions = -np.sort(-predictions)\n",
    "\n",
    "#    for i in range(pred.shape[1]):\n",
    "#        print(\"%s has confidence = %s\" % (classes[i], (predictions[i])))\n",
    "    \n",
    "    return classes[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "colab_type": "code",
    "id": "23VpGuihMdEU",
    "outputId": "cd36c932-0fb0-4166-92ae-546a7676e645"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "greet\n"
     ]
    }
   ],
   "source": [
    "text = \"Good morning\"\n",
    "pred = predictions(text)\n",
    "out = get_final_output(pred, intents_unique)\n",
    "print (out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cool recognized as confirm, correct is thanks\n",
      "Results: 345 from 346 correct = 99.71 percent\n"
     ]
    }
   ],
   "source": [
    "# with open('mowgli_test_new.csv', newline='') as csvfile:\n",
    "#    testfilelist = list(csv.reader(csvfile))\n",
    "\n",
    "with open('mowgli_train_new.csv', newline='') as csvfile:\n",
    "    testfilelist = list(csv.reader(csvfile))\n",
    "\n",
    "total   = 0\n",
    "correct = 0\n",
    "\n",
    "for s in testfilelist:\n",
    "    right_label = s[1]\n",
    "    message     = s[0]\n",
    "\n",
    "    pred   = predictions(message)\n",
    "    intent = get_final_output(pred, intents_unique)\n",
    "\n",
    "    if intent == right_label:\n",
    "        correct += 1\n",
    "    else:\n",
    "        print (\"%s recognized as %s, correct is %s\" % (message, intent, right_label))\n",
    "    total += 1\n",
    "\n",
    "print (\"Results: %d from %d correct = %4.2f percent\" % (correct, total, (correct/total)*100.0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Intent_classification_final.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
